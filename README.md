# Human Annotation of Prompt Translations

## Project Overview

This project enables human evaluation of prompt translations across multiple programming languages (Python, JavaScript, Java, PHP). The translations were generated using the latest generative AI models with internet access.

## Dataset Description

The datasets contain Python prompts collected from the literature, divided into two main sources:

1. **Stack Overflow**: Real-world prompts sourced from the literature, divided into:
   - Prompts collected before 2024
   - Prompts collected in 2024

2. **LLM-Generated Prompts**: Prompts generated by large language models (LLMs) based on descriptions of packages from PyPI, as reported in the literature, divided into:
   - Prompts generated before 2024
   - Prompts generated in 2024

Each Python prompt has been translated into JavaScript, Java, and PHP using the latest generative AI models with internet access.

## Repository Structure

```
.
├── Data/
│   ├── Java/
│   ├── JavaScript/
│   ├── Python/
│   └── Php/
├── results/
├── human_eval.py
├── README.md
```

- `Data/`: Contains the prompt datasets for each language.
- `results/`: Where your evaluation results will be saved automatically.
- `human_eval.py`: The script you will run to perform the evaluation.
- `README.md`: This guide.

## Instructions for Evaluators

### 1. Setup

- **Clone the repository** (or download the files).
- **Install Python 3** if not already installed. I think all of you work with PY :)

### 2. Running the Evaluation

Run the following command in your terminal:

```sh
python3 human_eval.py
```

### 3. Evaluation Process

- For each prompt, you will see the original Python prompt and its translation.
- Answer the questions about:
  - Faithfulness to the original prompt
  - Package existence
  - Overall quality
- If a package is hallucinated (doesn't exist in PyPI, npm, maven, ...) and ofc is not a standard library, specify the package(s) (comma-separated, no spaces).
- Your progress is saved automatically after each prompt.
- If you stop and restart the script, it will resume where you left off.

### 4. How Results Are Saved

- Results are saved in the `results/` directory.
- For each dataset and language, a file named `human_eval_<dataset>_<language>.json` is created (e.g., `human_eval_LLM_AT_Java.json`).
- Each file contains your answers for every prompt you evaluated.

### 5. Submitting Your Results

-It will start with JavaScript Dataset, PLEASE, when you are done with this one, send it to me. When you have finished your evaluation, **send me back the entire `results/` directory** (or just the `.json` files inside). 

## Additional Clarifications

- The evaluation will start with the JavaScript dataset. Once you finish this dataset, please send it to me. After completing the full evaluation, please send back the entire `results/` directory (or at least all the generated `.json` files).

- The Python repository collected from the literature may occasionally contain prompts that involve other programming languages, especially in the Stack Overflow subset. This is expected and does not necessarily indicate an error.

- In some cases, particularly for prompts involving packages, the mentioned package may refer to a broader ecosystem, framework, or tool rather than a precise standalone library. Please be understanding when evaluating such cases. Most importantly, the goal of this evaluation is to assess whether the translated prompt preserves the same task and intent, not whether it is a literal, word-for-word translation of the original Python prompt. A faithful adaptation that achieves the same objective in the target language is what matters most.

## Notes

- All paths are relative; you do not need to change any code or paths.
- You can interrupt and resume the evaluation at any time.
- Please answer carefully and consistently.

## Contact

For any questions or issues, please contact me. THANKSSSSSS A LOT FOR YOUR HELP <3
